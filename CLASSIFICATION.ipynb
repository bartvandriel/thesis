{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CLASSIFICATION.ipynb","version":"0.3.2","provenance":[{"file_id":"1Tc8XE5dM5HrdL5l7KA94CQ7vXcMSoFCt","timestamp":1544539927750},{"file_id":"1uxY33qCij0MztBsfXEgakueKU_jnmJZr","timestamp":1544016821333}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"3cvkJdcFFHQn","colab_type":"code","colab":{}},"cell_type":"code","source":["network = 'VGG'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IcebnIV5av5_","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras import backend as K\n","\n","from numpy.random import seed\n","seed(1)\n","from tensorflow import set_random_seed\n","set_random_seed(2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vNBonpXKSNpN","colab_type":"code","colab":{}},"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import itertools\n","from sklearn.metrics import confusion_matrix\n","\n","\n","def plot_confusion_matrix(cm, classes,\n","                          normalize=False,\n","                          title='Confusion matrix',\n","                          cmap=plt.cm.Blues):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","    \"\"\"\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        print(\"Normalized confusion matrix\")\n","    else:\n","        print('Confusion matrix, without normalization')\n","\n","#     print(cm)\n","\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45)\n","    plt.yticks(tick_marks, classes)\n","\n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, format(cm[i, j], fmt),\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","    plt.tight_layout()\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zzwfA5usHoD1","colab_type":"text"},"cell_type":"markdown","source":["**IMPORTS AND FUNCTIONS**"]},{"metadata":{"id":"ccgS7VPtdsi6","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.applications.vgg16 import  VGG16\n","from keras.applications.inception_v3 import InceptionV3\n","from keras.models import Model\n","from keras.layers import Dense, GlobalAveragePooling2D, Input, Flatten\n","from keras.optimizers import Adam, SGD\n","from keras.callbacks import Callback, CSVLogger, ModelCheckpoint\n","from keras.models import load_model\n","import numpy as np\n","\n","from tqdm import tqdm\n","\n","\n","import pandas as pd"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8AtFCHG0ZNG6","colab_type":"text"},"cell_type":"markdown","source":["**DOWNLOAD FILES**"]},{"metadata":{"id":"jwYM1ZG7ro-b","colab_type":"code","colab":{}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DMhNAPjc7SbU","colab_type":"text"},"cell_type":"markdown","source":["VGG"]},{"metadata":{"id":"BhENQkQI7KWY","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.preprocessing.image import ImageDataGenerator\n","import numpy as np\n","\n","\n","def preprocess_input_vgg(x):\n","      \"\"\"Wrapper around keras.applications.vgg16.preprocess_input()\n","      to make it compatible for use with keras.preprocessing.image.ImageDataGenerator's\n","      `preprocessing_function` argument.\n","\n","      Parameters\n","      ----------\n","      x : a numpy 3darray (a single image to be preprocessed)\n","\n","      Note we cannot pass keras.applications.vgg16.preprocess_input()\n","      directly to to keras.preprocessing.image.ImageDataGenerator's\n","      `preprocessing_function` argument because the former expects a\n","      4D tensor whereas the latter expects a 3D tensor. Hence the\n","      existence of this wrapper.\n","\n","      Returns a numpy 3darray (the preprocessed image).\n","\n","      \"\"\"\n","      from keras.applications.vgg16 import preprocess_input\n","      X = np.expand_dims(x, axis=0)\n","      X = preprocess_input(X)\n","      return X[0]\n","\n","\n","def get_data_generators(file_num, network, file_name):\n","\n","  \n","  if network == 'VGG':\n","    preprocess_input = preprocess_input_vgg\n","    input_size = (224, 224)\n","    print(\"VGG DATA GENERATOR\")\n","  elif network == 'Xception':\n","    from keras.applications.xception import preprocess_input \n","    print(\"Xception DATA GENERATOR\")\n","    input_size = (299, 299)\n","  elif network == 'Inception':\n","    from keras.applications.inception_v3 import preprocess_input\n","    print(\"Inception DATA GENERATOR\")\n","    input_size = (299, 299)\n","    \n","  \n","  \n","  \n","  import os\n","  import zipfile\n","\n","  local_zip = 'gdrive/My Drive/Colab Notebooks/UC_Merced/' + file_name\n","#   local_zip = 'gdrive/My Drive/Colab Notebooks/Sint_Maarten/' + 'shape/' + str(10) + '/'+ file_name\n","#   local_zip = 'gdrive/My Drive/Colab Notebooks/Sint_Maarten/' + 'material/' + str(10) + '/'+ file_name\n","\n","  print(local_zip)\n","  zip_ref = zipfile.ZipFile(local_zip, 'r')\n","  zip_ref.extractall('/tmp')\n","  zip_ref.close()\n","\n","  base_dir = '/tmp/' + file_name.split(\".\")[0]\n","  # base_dir = '/tmp/UC_Merced_filtered_45'\n","\n","  print(base_dir)\n","\n","  train_dir = os.path.join(base_dir, 'train')\n","  validation_dir = os.path.join(base_dir, 'validation')\n","  test_dir = os.path.join(base_dir, 'test')\n","\n","\n","  # Adding rescale, rotation_range, width_shift_range, height_shift_range,\n","  # shear_range, zoom_range, and horizontal flip to our ImageDataGenerator\n","  train_datagen = ImageDataGenerator(\n","      preprocessing_function=preprocess_input,\n","      rotation_range=180,\n","      horizontal_flip=True,\n","      vertical_flip=True\n","  )\n","  val_datagen = ImageDataGenerator(\n","      preprocessing_function=preprocess_input,\n","      rotation_range=180,\n","      horizontal_flip=True,\n","      vertical_flip=True\n","  )\n","  # Note that the validation and test data should not be augmented!\n","  test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n","\n","  # Flow training images in batches of 16 using train_datagen generator\n","  train_generator = train_datagen.flow_from_directory(\n","          train_dir,  # This is the source directory for training images\n","          target_size=input_size,  # All images will be resized to 150x150\n","          batch_size=16,\n","          # Since we use binary_crossentropy loss, we need binary labels\n","          class_mode='sparse', \n","          shuffle=True)\n","\n","  # Flow validation images in batches of 16 using test_datagen generator\n","  validation_generator = val_datagen.flow_from_directory(\n","          validation_dir,\n","          target_size=input_size,\n","          batch_size=16,\n","          class_mode='sparse')\n","\n","  # Flow test images in batches of 16 using test_datagen generator\n","  test_generator = test_datagen.flow_from_directory(\n","          test_dir,\n","          target_size=input_size,\n","          batch_size=16,\n","          class_mode='sparse')\n","  \n","  return train_generator, validation_generator, test_generator\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SxKbEK3NNgHV","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","from keras.preprocessing.image import ImageDataGenerator\n","import numpy as np\n","\n","def get_Xception_data_generator(file_num, file_name):\n","  from keras.applications.xception import preprocess_input\n","  \n","  \n","  print('Xception DATA GENERATOR')\n","    \n","  import os\n","  import zipfile\n","#   local_zip = 'gdrive/My Drive/Colab Notebooks/UC_Merced/' + file_name\n","#   local_zip = 'gdrive/My Drive/Colab Notebooks/Sint_Maarten/shape/' + file_name\n","  local_zip = 'gdrive/My Drive/Colab Notebooks/Sint_Maarten/material/' + file_name\n","\n","\n","  print(local_zip)\n","  zip_ref = zipfile.ZipFile(local_zip, 'r')\n","  zip_ref.extractall('/tmp')\n","  zip_ref.close()\n","\n","  base_dir = '/tmp/' + file_name.split(\".\")[0]\n","  # base_dir = '/tmp/UC_Merced_filtered_45'\n","\n","  print(base_dir)\n","\n","  train_dir = os.path.join(base_dir, 'train')\n","  validation_dir = os.path.join(base_dir, 'validation')\n","  test_dir = os.path.join(base_dir, 'test')\n","\n","\n","  # Adding rescale, rotation_range, width_shift_range, height_shift_range,\n","  # shear_range, zoom_range, and horizontal flip to our ImageDataGenerator\n","  train_datagen = ImageDataGenerator(\n","      preprocessing_function=preprocess_input,\n","      rotation_range=180,\n","      horizontal_flip=True,\n","      vertical_flip=True)\n","\n","  # Note that the validation and test data should not be augmented!\n","  val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n","  test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n","\n","  # Flow training images in batches of 16 using train_datagen generator\n","  train_generator = train_datagen.flow_from_directory(\n","          train_dir,  # This is the source directory for training images\n","          target_size=(299, 299),  # All images will be resized to 150x150\n","          batch_size=16,\n","          # Since we use binary_crossentropy loss, we need binary labels\n","          class_mode='sparse', \n","          shuffle=True)\n","\n","  # Flow validation images in batches of 16 using test_datagen generator\n","  validation_generator = val_datagen.flow_from_directory(\n","          validation_dir,\n","          target_size=(299, 299),\n","          batch_size=16,\n","          class_mode='sparse')\n","\n","  # Flow test images in batches of 16 using test_datagen generator\n","  test_generator = test_datagen.flow_from_directory(\n","          test_dir,\n","          target_size=(299, 299),\n","          batch_size=100,\n","          class_mode='sparse')\n","  \n","  return train_generator, validation_generator, test_generator\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"u5ZvZFCekrAr","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.metrics import precision_recall_fscore_support, accuracy_score, cohen_kappa_score\n","\n","def eval(prediction_lists, label_lists):\n","\n","  precision_list = []\n","  recall_list = []\n","  fscore_list = []\n","  support_list = []\n","  accuracy_list =[]\n","  kappa_list = []\n","\n","\n","\n","  for prediction_list, label_list in zip(prediction_lists, label_lists): \n","\n","    precision, recall, fscore, support= precision_recall_fscore_support(label_list, prediction_list)\n","    accuracy = accuracy_score(label_list, prediction_list)\n","    kappa = cohen_kappa_score(label_list, prediction_list)\n","\n","    precision_list.append(precision)\n","    recall_list.append(recall)\n","    fscore_list.append(fscore)\n","    support_list.append(support)\n","    accuracy_list.append(accuracy)\n","    kappa_list.append(kappa)\n","\n","  precision_list = np.asarray(precision_list)\n","  recall_list = np.asarray(recall_list)\n","  fscore_list = np.asarray(fscore_list)\n","  support_list = np.asarray(support_list)\n","  accuracy_list = np.asarray(accuracy_list)\n","  kappa_list = np.asarray(kappa_list) \n","\n","  precision = [np.mean(precision_list, axis=0), np.std(precision_list, axis=0)]\n","  recall = [np.mean(recall_list, axis=0),  np.std(recall_list, axis=0)]\n","  fscore = [np.mean(fscore_list, axis=0), np.std(fscore_list, axis=0)]\n","  support = [np.mean(support_list, axis=0), np.std(support_list, axis=0)]\n","  accuracy = [np.mean(accuracy_list, axis=0), np.std(accuracy_list, axis=0)]\n","  kappa = [np.mean(kappa_list, axis=0),  np.std(kappa_list, axis=0)]\n","\n","  return precision, recall, fscore, support, accuracy, kappa\n","  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"sscxRbd2SWQ1","colab_type":"text"},"cell_type":"markdown","source":["**Load models**"]},{"metadata":{"id":"GraImJjb4Hip","colab_type":"code","colab":{}},"cell_type":"code","source":["import os\n","\n","PATH = '/content/gdrive/My Drive/Colab Notebooks/RUNS/UC_Merced/models/' + network +'/trained_models/'\n","# PATH = '/content/gdrive/My Drive/Colab Notebooks/RUNS/Saint_Martin/Shape/models/' + network +'/trained_models/'\n","# PATH = '/content/gdrive/My Drive/Colab Notebooks/RUNS/Saint_Martin/Material/models/' + network +'/trained_models/'\n","\n","\n","\n","model_names = os.listdir(PATH)\n","model_names.sort()\n","models = []\n","\n","for name in model_names:\n","  \n","  file_path = os.path.join(PATH, name)\n","  print(file_path)\n","  models.append(load_model(file_path))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BYZpIQGZ4n4u","colab_type":"text"},"cell_type":"markdown","source":["**Predictions**"]},{"metadata":{"id":"i7IW-fV8C6PR","colab_type":"code","colab":{}},"cell_type":"code","source":["df = pd.DataFrame(columns=['network', 'classifier', 'best_parameters', 'precision', 'recall', 'fscore', 'support', 'accuracy', 'kappa', 'labels', 'predictions'])\n","  \n","columns = list(df.columns)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CYEMa8E_SedH","colab_type":"text"},"cell_type":"markdown","source":["**Classification**"]},{"metadata":{"id":"KX38Gm3QSpO6","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_data(num, models):\n","  \n","  base_model =  models[num]\n","  model = Model(inputs=base_model.input, outputs=base_model.get_layer('fc2').output)\n","  \n","  if num == 0 and len(models) ==  10:\n","    num = 10\n","  #   Filename\n","  file_name = 'UC_Merced_filtered_'+ str(num) +'.zip'\n","#   file_name = 'roof_shape_'+ str(num*10) +'_0_9' + '.zip'\n","#   file_name = 'roof_material_'+ str(num*10) +'_0_9' + '.zip'\n","\n","  #generators\n","  train_generator, validation_generator, test_generator = get_data_generators(num, network, file_name)\n","\n","  #Create trainings data\n","  images_train = []\n","  labels_train = []\n","  \n","  for i in range(2*int(np.ceil(train_generator.samples/train_generator.batch_size))):\n","    images, labels = train_generator.next()\n","    images = model.predict(images)\n","\n","    for image, label in zip(images, labels):\n","      images_train.append(image)\n","      labels_train.append(label)\n","\n","  images_train = np.asarray(images_train)\n","  labels_train = np.asarray(labels_train)\n","  \n","\n","  \n","  #Create validation data\n","  images_val = []\n","  labels_val = []\n","  \n","  for i in range(1 *int(np.ceil(validation_generator.samples/validation_generator.batch_size))):\n","    images, labels = validation_generator.next()\n","    images = model.predict(images)\n","\n","    for image, label in zip(images, labels):\n","      images_val.append(image)\n","      labels_val.append(label)\n","\n","  images_val = np.asarray(images_val)\n","  labels_val = np.asarray(labels_val)\n","\n","  \n","  #Create test data\n","  images_test = []\n","  labels_test = []\n","  \n","  for i in range(int(1 *np.ceil(test_generator.samples/test_generator.batch_size))):\n","    images, labels = test_generator.next()\n","    images = model.predict(images)\n","\n","    for image, label in zip(images, labels):\n","      images_test.append(image)\n","      labels_test.append(label)\n","\n","  images_test = np.asarray(images_test)\n","  labels_test = np.asarray(labels_test)\n","  \n","  return images_train, labels_train, images_val, labels_val, images_test, labels_test\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AepCiVW9eKCM","colab_type":"code","colab":{}},"cell_type":"code","source":["param_dist = {\"n_estimators\": [100, 200, 400, 800, 1600],\n","          \"min_samples_split\":  [2, 5, 10],\n","          \"min_samples_leaf\": [1, 2, 4],\n","          \"max_features\":['sqrt', 'log2'],\n","          \"max_depth\": [20, 40, 60, 80, 100, None]}\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RJR4NlHFX382","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.model_selection import RandomizedSearchCV\n","from sklearn.model_selection import GridSearchCV\n","\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import LinearSVC\n","\n","\n","file_name = 'Material_SVM.pkl'\n","\n","\n","i = 0\n","for clf_name in ['svm']:\n","  \n","  prediction_lists = []\n","  label_lists = []\n","  best_parameters_lists = []\n","  \n","  for num in range(10):\n","    \n","    #data\n","    images_train, labels_train, images_val, labels_val, images_test, labels_test = get_data(num, models)\n","    \n","    images_train = np.concatenate((images_train, images_val), axis=0)\n","    labels_train = np.concatenate((labels_train, labels_val), axis=0)\n","    \n","    print(images_train.shape, images_test.shape)\n","    \n","    if clf_name == 'rf':\n","      \n","      param_dist = {\"n_estimators\": [100, 200, 400, 800, 1600],\n","                    \"min_samples_split\":  [2, 5, 10],\n","                    \"min_samples_leaf\": [1, 2, 4],\n","                    \"max_features\":['sqrt', 'log'],\n","                    \"max_depth\": [20, 40, 60, 80, 100, None]}\n","      \n","      clf = RandomForestClassifier()\n","      search = RandomizedSearchCV(clf, param_distributions=param_dist,\n","                                     n_iter=60, cv=3)\n","      \n","      search.fit(images_train, labels_train)\n","      parameters =  list(search.best_params_ .values())\n","      \n","    elif clf_name == 'svm':\n","      \n","      search = LinearSVC(max_iter=500*5000, C = 1, multi_class='ovr')\n","      search.fit(images_train, labels_train)\n","      parameters =  1\n","\n","    #Predict using classifier\n","    prediction_list = search.predict(images_test)\n","    label_list = labels_test\n","\n","    prediction_lists.append(np.asarray(prediction_list))\n","\n","\n","    \n","    label_lists.append(np.asarray(label_list))\n","    best_parameters_lists.append(parameters)\n","    \n","    \n","  #Create evaluation metrics  \n","  precision, recall, fscore, support, accuracy, kappa =  eval(prediction_lists, label_lists)\n","\n","  labels = np.asarray(label_lists).flatten()\n","  predictions = np.asarray(prediction_lists).flatten()\n","\n","  print(i, clf_name, accuracy[0])\n","  i += 1\n","\n","  #Add to dataframe\n","  obs = pd.DataFrame([[network, clf_name, best_parameters_lists,  precision, recall, fscore, support, accuracy, kappa, labels, predictions]], columns=columns)\n","  df = df.append(obs)\n","  df.to_pickle('/content/gdrive/My Drive/Colab Notebooks/Sint_Maarten/pickles/classification/' + file_name)\n","\n"],"execution_count":0,"outputs":[]}]}