{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CODING.ipynb","version":"0.3.2","provenance":[{"file_id":"1Tc8XE5dM5HrdL5l7KA94CQ7vXcMSoFCt","timestamp":1544539927750},{"file_id":"1uxY33qCij0MztBsfXEgakueKU_jnmJZr","timestamp":1544016821333}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"3cvkJdcFFHQn","colab_type":"code","colab":{}},"cell_type":"code","source":["network = 'VGG'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IcebnIV5av5_","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","from numpy.random import seed\n","seed(1)\n","from tensorflow import set_random_seed\n","set_random_seed(2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zzwfA5usHoD1","colab_type":"text"},"cell_type":"markdown","source":["**IMPORTS AND FUNCTIONS**"]},{"metadata":{"id":"ccgS7VPtdsi6","colab_type":"code","outputId":"5cd8baa0-b7d0-43fa-b46f-8abd179d5c98","executionInfo":{"status":"ok","timestamp":1549351297639,"user_tz":-60,"elapsed":3620,"user":{"displayName":"Bart van Driel","photoUrl":"","userId":"17232338587989471094"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["from keras.applications.vgg16 import  VGG16\n","from keras.applications.inception_v3 import InceptionV3\n","from keras.models import Model\n","from keras.layers import Dense, GlobalAveragePooling2D, Input, Flatten\n","from keras.optimizers import SGD\n","from keras.callbacks import Callback, CSVLogger, ModelCheckpoint\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.models import load_model\n","import numpy as np\n","\n","from tqdm import tqdm\n","\n","\n","import pandas as pd"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"et7_ExV-i9mT","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.cluster import MiniBatchKMeans\n","from sklearn.preprocessing import normalize\n","from sklearn.svm import SVC\n","from scipy.spatial.distance import cdist\n","from sklearn.mixture import GaussianMixture"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8AtFCHG0ZNG6","colab_type":"text"},"cell_type":"markdown","source":["**DOWNLOAD FILES**"]},{"metadata":{"id":"jwYM1ZG7ro-b","colab_type":"code","outputId":"e7fd78e5-30ff-4ed6-dcae-fe4a19bb1f44","executionInfo":{"status":"ok","timestamp":1549351350857,"user_tz":-60,"elapsed":56774,"user":{"displayName":"Bart van Driel","photoUrl":"","userId":"17232338587989471094"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"metadata":{"id":"DMhNAPjc7SbU","colab_type":"text"},"cell_type":"markdown","source":["VGG"]},{"metadata":{"id":"qTS-_0G1Gk_6","colab_type":"text"},"cell_type":"markdown","source":["**CNN HELP FUNCTIONS**"]},{"metadata":{"id":"BhENQkQI7KWY","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.preprocessing.image import ImageDataGenerator\n","import numpy as np\n","\n","\n","def preprocess_input_vgg(x):\n","      \"\"\"Wrapper around keras.applications.vgg16.preprocess_input()\n","      to make it compatible for use with keras.preprocessing.image.ImageDataGenerator's\n","      `preprocessing_function` argument.\n","\n","      Parameters\n","      ----------\n","      x : a numpy 3darray (a single image to be preprocessed)\n","\n","      Note we cannot pass keras.applications.vgg16.preprocess_input()\n","      directly to to keras.preprocessing.image.ImageDataGenerator's\n","      `preprocessing_function` argument because the former expects a\n","      4D tensor whereas the latter expects a 3D tensor. Hence the\n","      existence of this wrapper.\n","\n","      Returns a numpy 3darray (the preprocessed image).\n","\n","      \"\"\"\n","      from keras.applications.vgg16 import preprocess_input\n","      X = np.expand_dims(x, axis=0)\n","      X = preprocess_input(X)\n","      return X[0]\n","\n","\n","def get_VGG_data_generator(file_num, file_name):\n","  print('VGG DATA GENERATOR')\n","\n","\n","  import os\n","  import zipfile\n","#   local_zip = 'gdrive/My Drive/Colab Notebooks/Sint_Maarten/' + 'shape/' + str(10) + '/'+ file_name\n","  local_zip = 'gdrive/My Drive/Colab Notebooks/Sint_Maarten/' + 'material/' + str(10) + '/'+ file_name\n","\n","#   local_zip = 'gdrive/My Drive/Colab Notebooks/UC_Merced/' + file_name\n","\n","  print(local_zip)\n","  zip_ref = zipfile.ZipFile(local_zip, 'r')\n","  zip_ref.extractall('/tmp')\n","  zip_ref.close()\n","\n","  base_dir = '/tmp/' + file_name.split(\".\")[0]\n","  # base_dir = '/tmp/UC_Merced_filtered_45'\n","\n","  print(base_dir)\n","\n","  train_dir = os.path.join(base_dir, 'train')\n","  validation_dir = os.path.join(base_dir, 'validation')\n","  test_dir = os.path.join(base_dir, 'test')\n","\n","\n","  # Adding rescale, rotation_range, width_shift_range, height_shift_range,\n","  # shear_range, zoom_range, and horizontal flip to our ImageDataGenerator\n","  train_datagen = ImageDataGenerator(\n","      preprocessing_function=preprocess_input_vgg,\n","      rotation_range=180,\n","      horizontal_flip=True,\n","      vertical_flip=True\n","  )\n","  val_datagen = ImageDataGenerator(\n","      preprocessing_function=preprocess_input_vgg,\n","      rotation_range=180,\n","      horizontal_flip=True,\n","      vertical_flip=True\n","  )\n","\n","  # Note that the validation and test data should not be augmented!\n","  test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input_vgg)\n","\n","  # Flow training images in batches of 16 using train_datagen generator\n","  train_generator = train_datagen.flow_from_directory(\n","          train_dir,  # This is the source directory for training images\n","          target_size=(224, 224),  # All images will be resized to 150x150\n","          batch_size=16,\n","          # Since we use binary_crossentropy loss, we need binary labels\n","          class_mode='sparse', \n","          shuffle=True)\n","\n","  # Flow validation images in batches of 16 using test_datagen generator\n","  validation_generator = val_datagen.flow_from_directory(\n","          validation_dir,\n","          target_size=(224, 224),\n","          batch_size=16,\n","          class_mode='sparse',\n","           shuffle=True)\n","\n","  # Flow test images in batches of 16 using test_datagen generator\n","  test_generator = test_datagen.flow_from_directory(\n","          test_dir,\n","          target_size=(224, 224),\n","          batch_size=100,\n","          class_mode='sparse')\n","  \n","  return train_generator, validation_generator, test_generator\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nptxiOk3fVHK","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_cnn_features(generator, model):\n","  \n","  features = []\n","  labels = []\n","  for i in range(int(np.ceil(generator.samples/generator.batch_size))):\n","    images, label_indices = generator.next()\n","    images = model.predict(images)\n","\n","    for image, label in zip(images, label_indices):\n","      features.append(image)\n","      labels.append(label)\n","\n","  features = np.asarray(features)\n","  labels = np.asarray(labels)\n","\n","  return features, labels"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7mtSYzluGzuC","colab_type":"code","colab":{}},"cell_type":"code","source":["def set_data_generators(file_num, network, file_name):\n","  \n","  if network == 'VGG':\n","    train_generator, validation_generator, test_generator = get_VGG_data_generator(file_num, file_name)\n","    \n","  elif network == 'Inception':\n","    train_generator, validation_generator, test_generator = get_Inception_data_generator(file_num, file_name)\n","    \n","  elif network == 'Xception':\n","    train_generator, validation_generator, test_generator = get_Xception_data_generator(file_num, file_name)\n","  \n","    \n","  return train_generator, validation_generator, test_generator"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sscxRbd2SWQ1","colab_type":"text"},"cell_type":"markdown","source":["**EVALUATION FUNCTIONS**"]},{"metadata":{"id":"BgcXnf8_6iKY","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.metrics import precision_recall_fscore_support, accuracy_score, cohen_kappa_score\n","\n","def eval(prediction_lists, label_lists):\n","\n","  precision_list = []\n","  recall_list = []\n","  fscore_list = []\n","  support_list = []\n","  accuracy_list =[]\n","  kappa_list = []\n","\n","\n","\n","  for prediction_list, label_list in zip(prediction_lists, label_lists): \n","\n","    precision, recall, fscore, support= precision_recall_fscore_support(label_list, prediction_list)\n","    accuracy = accuracy_score(label_list, prediction_list)\n","    kappa = cohen_kappa_score(label_list, prediction_list)\n","\n","    precision_list.append(precision)\n","    recall_list.append(recall)\n","    fscore_list.append(fscore)\n","    support_list.append(support)\n","    accuracy_list.append(accuracy)\n","    kappa_list.append(kappa)\n","\n","  precision_list = np.asarray(precision_list)\n","  recall_list = np.asarray(recall_list)\n","  fscore_list = np.asarray(fscore_list)\n","  support_list = np.asarray(support_list)\n","  accuracy_list = np.asarray(accuracy_list)\n","  kappa_list = np.asarray(kappa_list) \n","\n","  precision = [np.mean(precision_list, axis=0), np.std(precision_list, axis=0)]\n","  recall = [np.mean(recall_list, axis=0),  np.std(recall_list, axis=0)]\n","  fscore = [np.mean(fscore_list, axis=0), np.std(fscore_list, axis=0)]\n","  support = [np.mean(support_list, axis=0), np.std(support_list, axis=0)]\n","  accuracy = [np.mean(accuracy_list, axis=0), np.std(accuracy_list, axis=0)]\n","  kappa = [np.mean(kappa_list, axis=0),  np.std(kappa_list, axis=0)]\n","\n","  return precision, recall, fscore, support, accuracy, kappa"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fUwyafYOGxHc","colab_type":"text"},"cell_type":"markdown","source":["**CODING FUNCTIONS**"]},{"metadata":{"id":"FqNJdXKYg58u","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","def get_BOW_features(images, kmeans):\n","\n","  histo = np.zeros((images.shape[0], kmeans.n_clusters))\n","  for i, image in tqdm(enumerate(images)):\n","    image = image.reshape(np.prod(image.shape[:-1]), image.shape[-1])\n","    image = normalize(image)\n","    words = kmeans.predict(image)\n","    for word in words:\n","      histo[i][word] += 1\n","      \n","  histo = normalize(histo)  \n","  \n","  return histo"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-hh5xRSQG5GB","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_VLAD_features(images, kmeans):\n","  \n","  centers = kmeans.cluster_centers_\n","  k=kmeans.n_clusters\n","  V_list = []\n","  for image in (images):\n","    image = image.reshape(np.prod(image.shape[:-1]), image.shape[-1])\n","    m, d = image.shape\n","    words = kmeans.predict(image)\n","\n","\n","    V=np.zeros([k,d])\n","\n","    for i in range(k):\n","\n","      if np.sum(words == i) > 0:\n","        V[i]=np.sum(image[words==i,:]-centers[i],axis=0)\n","\n","\n","    V = V.flatten()\n","    # power normalization, also called square-rooting normalization\n","    V = np.sign(V)*np.sqrt(np.abs(V))\n","\n","    # L2 normalization\n","\n","    V = V/np.sqrt(np.dot(V,V))\n","\n","    V_list.append(V)\n","\n","  V_list = np.asarray(V_list) \n","  \n","  return V_list\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"c8CGDi-VHSom","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_LLC_features(images, kmeans):\n","  \n","  LLC_features = []\n","  for X in (images):\n","    X = X.reshape(np.prod(X.shape[:-1]), X.shape[-1])\n","    X = normalize(X)\n","\n","    C = kmeans.cluster_centers_\n","    N, d = X.shape\n","    c, _ = C.shape\n","    k = int(0.1*c)\n","    beta = 1e-6\n","    \n","\n","    D = cdist(X, C, 'euclidean')\n","    I = np.zeros((N, k),dtype=int)\n","\n","    for i in range(N):\n","      d = D[i, :]\n","      idx = np.argsort(d)\n","      I[i, :] = idx[:k]\n","\n","\n","      II = np.eye(k)\n","\n","      G = np.zeros((N, c)) #Gammas\n","      ones = np.ones((k,1))\n","\n","    for i in range(N):\n","      idx = I[i,:]\n","      z = C[idx,:] - np.tile(X[i,:], (k, 1))  # shift ith pt to origin\n","      Z = np.dot(z,z.T)                       # local covariance\n","      Z = Z + II*beta*np.trace(Z);            # regularlization (K>D)\n","      w = np.linalg.solve(Z,ones)     #np.dot(np.linalg.inv(Z), ones)\n","      w = w/np.sum(w)                         # enforce sum(w)=1       \n","      G[i,idx] = w.ravel()\n","#       Y = np.dot(G,C)\n","    LLC_features.append(G)\n","    \n","  LLC_features = np.asarray(LLC_features)\n","  LLC_features= LLC_features.reshape(LLC_features.shape[0], np.prod(LLC_features.shape[1:]))\n","    \n","  return LLC_features"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_dBx0o4cG4fl","colab_type":"code","colab":{}},"cell_type":"code","source":[" def get_IFK_features(images, gmm):\n","    \n","    means = gmm.means_\n","    weights = gmm.weights_\n","    n_kernels = gmm.n_components\n","    covars = gmm.covariances_\n","\n","    cov_matrices = np.empty(shape=(n_kernels, covars.shape[1], covars.shape[1]))\n","    for i in range(n_kernels):\n","      cov_matrices[i, :, :] = np.diag(covars[i, :])\n","    covars = cov_matrices\n","    \n","    n_images = images.shape[0]\n","    n_iter = int(np.ceil(n_images/10))\n","    \n","    fisher_features = []\n","    for i in (range(1, n_iter + 1)):\n","      \n","      up = min(n_images, 10*i)\n","      X = images[(i-1)*10:up]\n","\n","      X = X.reshape(1, X.shape[0],np.prod(X.shape[1:3]), X.shape[3])\n","\n","\n","      assert X.ndim == 4\n","\n","      n_videos, n_frames = X.shape[0], X.shape[1]\n","\n","      X = X.reshape((-1, X.shape[-2], X.shape[-1])) #(n_images, n_features, n_feature_dim)\n","      X_matrix = X.reshape(-1, X.shape[-1])\n","      X_matrix = normalize(X_matrix)\n","      \n","\n","      # set equal weights to predict likelihood ratio\n","      gmm.weights_ = np.ones(n_kernels) / n_kernels\n","      likelihood_ratio = gmm.predict_proba(X_matrix).reshape(X.shape[0], X.shape[1], n_kernels)\n","\n","      var = np.diagonal(covars, axis1=1, axis2=2)\n","\n","      norm_dev_from_modes = ((X[:,:, None, :] - means[None, None, :, :])/ var[None, None, :, :]) # (n_images, n_features, n_kernels, n_featur_dim)\n","\n","      # mean deviation\n","      mean_dev = np.multiply(likelihood_ratio[:,:,:, None], norm_dev_from_modes).mean(axis=1) #(n_images, n_kernels, n_feature_dim)\n","      mean_dev = np.multiply(1 / np.sqrt(weights[None, :,  None]), mean_dev) #(n_images, n_kernels, n_feature_dim)\n","\n","      # covariance deviation\n","      cov_dev = np.multiply(likelihood_ratio[:,:,:, None], norm_dev_from_modes**2 - 1).mean(axis=1)\n","      cov_dev = np.multiply(1 / np.sqrt(2 *weights[None, :,  None]), cov_dev)\n","\n","      fisher_vectors = np.concatenate([mean_dev, cov_dev], axis=1)\n","\n","      # final reshape - separate frames and videos\n","      assert fisher_vectors.ndim == 3\n","      fisher_vectors = fisher_vectors.reshape((n_videos, n_frames, fisher_vectors.shape[1], fisher_vectors.shape[2]))\n","\n","\n","      fisher_vectors = np.sqrt(np.abs(fisher_vectors)) * np.sign(fisher_vectors) # power normalization\n","      fisher_vectors = fisher_vectors / np.linalg.norm(fisher_vectors, axis=(2,3))[:,:,None,None]\n","\n","      fisher_vectors[fisher_vectors < 10**-4] = 0\n","      \n","      for image in fisher_vectors[0]:\n","        fisher_features.append(image)\n","        \n","        \n","    fisher_features = np.asarray(fisher_features)\n","    \n","    fisher_features = fisher_features.reshape(fisher_features.shape[0], np.prod(fisher_features.shape[1:]))\n","    \n","    return fisher_features"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BYZpIQGZ4n4u","colab_type":"text"},"cell_type":"markdown","source":["**LOAD MODELS**"]},{"metadata":{"id":"GraImJjb4Hip","colab_type":"code","outputId":"88b03c36-9eb5-4620-8a20-b03639f33d6f","executionInfo":{"status":"ok","timestamp":1549351427382,"user_tz":-60,"elapsed":133185,"user":{"displayName":"Bart van Driel","photoUrl":"","userId":"17232338587989471094"}},"colab":{"base_uri":"https://localhost:8080/","height":289}},"cell_type":"code","source":["import os\n","# PATH = '/content/gdrive/My Drive/Colab Notebooks/Sint_Maarten/pickles/models/trained_models/UC_Merced/'\n","# PATH = '/content/gdrive/My Drive/Colab Notebooks/Sint_Maarten/pickles/models/trained_models/Shape/'\n","PATH = '/content/gdrive/My Drive/Colab Notebooks/Sint_Maarten/pickles/models/trained_models/Material/'\n","\n","model_names = os.listdir(PATH)\n","model_names.sort()\n","\n","models = []\n","\n","for name in model_names:\n","  \n","  file_path = os.path.join(PATH, name)\n","  print(file_path)\n","  models.append(load_model(file_path))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/Colab Notebooks/Sint_Maarten/pickles/models/trained_models/Material/3_roof_material_100_0_9\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","/content/gdrive/My Drive/Colab Notebooks/Sint_Maarten/pickles/models/trained_models/Material/3_roof_material_10_0_9\n","/content/gdrive/My Drive/Colab Notebooks/Sint_Maarten/pickles/models/trained_models/Material/3_roof_material_20_0_9\n","/content/gdrive/My Drive/Colab Notebooks/Sint_Maarten/pickles/models/trained_models/Material/3_roof_material_30_0_9\n","/content/gdrive/My Drive/Colab Notebooks/Sint_Maarten/pickles/models/trained_models/Material/3_roof_material_40_0_9\n","/content/gdrive/My Drive/Colab Notebooks/Sint_Maarten/pickles/models/trained_models/Material/3_roof_material_50_0_9\n","/content/gdrive/My Drive/Colab Notebooks/Sint_Maarten/pickles/models/trained_models/Material/3_roof_material_60_0_9\n","/content/gdrive/My Drive/Colab Notebooks/Sint_Maarten/pickles/models/trained_models/Material/3_roof_material_70_0_9\n","/content/gdrive/My Drive/Colab Notebooks/Sint_Maarten/pickles/models/trained_models/Material/3_roof_material_80_0_9\n","/content/gdrive/My Drive/Colab Notebooks/Sint_Maarten/pickles/models/trained_models/Material/3_roof_material_90_0_9\n"],"name":"stdout"}]},{"metadata":{"id":"ug1wiP1dHgZm","colab_type":"text"},"cell_type":"markdown","source":["**LOAD/CREATE DATAFRMES**"]},{"metadata":{"id":"EV7wb6sZ6-3L","colab_type":"code","outputId":"a7ff4b82-1f97-4eef-be20-7b14671140e5","executionInfo":{"status":"ok","timestamp":1549351429155,"user_tz":-60,"elapsed":134941,"user":{"displayName":"Bart van Driel","photoUrl":"","userId":"17232338587989471094"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["df_file_name = 'coding_df_Material'\n","PATH_df = '/content/gdrive/My Drive/Colab Notebooks/Sint_Maarten/pickles/classification/'\n","if os.path.isfile(PATH_df + df_file_name):\n","  df = pd.read_pickle(PATH_df + df_file_name)\n","  print('LOAD DF')\n","else:\n","  df = pd.DataFrame(columns=['network', 'coding', 'par1', 'precision', 'recall', 'fscore', 'support', 'accuracy', 'kappa', 'labels', 'predictions'])\n","  print('CREATE NEW DF')\n","  \n","columns = list(df.columns)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["LOAD DF\n"],"name":"stdout"}]},{"metadata":{"id":"HXb7vRSnPtfE","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_coded_features(coding, images, cluster):\n","  \n","  if coding == 'BOW':\n","    features =  get_BOW_features(images, cluster)\n","    \n","  if coding == 'IFK':\n","    features =  get_IFK_features(images, cluster)\n","    \n","  if coding == 'VLAD':\n","    features =  get_VLAD_features(images, cluster)\n","    \n","  if coding == 'LLC':\n","    features =  get_LLC_features(images, cluster)\n","    \n","  return features\n","\n","\n","def get_parameters(coding):\n","  if coding == 'BOW':\n","    parameters =  [1000]\n","    \n","  if coding == 'IFK':\n","    parameters =  [100]\n","    \n","  if coding == 'VLAD':\n","    parameters =  [100]\n","    \n","  if coding == 'LLC':\n","    parameters =  [100]\n","    \n","  if coding == 'None':\n","    parameters =  ['None']\n","    \n","    \n","  return parameters "],"execution_count":0,"outputs":[]},{"metadata":{"id":"YrkQ9KwSNVhx","colab_type":"code","colab":{}},"cell_type":"code","source":["import pickle"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PMh_wNISb-gi","colab_type":"code","outputId":"47ecb7d6-1afd-4ba6-dbe5-8ef5b0870f94","executionInfo":{"status":"ok","timestamp":1549375853824,"user_tz":-60,"elapsed":6107909,"user":{"displayName":"Bart van Driel","photoUrl":"","userId":"17232338587989471094"}},"colab":{"base_uri":"https://localhost:8080/","height":306}},"cell_type":"code","source":["from sklearn.svm import LinearSVC\n","\n","for coding in ['IFK']:\n","  print('CODING: ', coding)\n","  \n","  for par1 in get_parameters(coding):\n","    print('PARAMETER: ', par1 )\n","    #Predict\n","    prediction_lists = []\n","    label_lists = []\n","\n","    for num in range(len(models)):\n","#     for num in range(2, 3):\n","      print('FOLD', num + 1, 'out of', len(models))\n","      model = models[num]\n","      model = Model(inputs=model.input, outputs=model.get_layer('block5_conv3').output)\n","\n","      \n","      if num == 0 and len(models) ==  10:\n","        num = 10\n","        \n","      #Filename\n","#       file_name = 'UC_Merced_filtered_'+ str(num) +'.zip'\n","#       file_name = 'roof_shape_'+ str(num*10) +'_0_9' + '.zip'\n","      file_name = 'roof_material_'+ str(num*10) +'_0_9' + '.zip'\n","\n","      #generators\n","      train_generator, validation_generator, test_generator = set_data_generators(num, network, file_name)\n","    \n","      if coding != 'None':\n","\n","        #get training data\n","        x_train, y_train = get_cnn_features(train_generator, model)\n","\n","        print(x_train.shape)\n","        #local features \n","        local_features = x_train\n","        local_features = np.reshape(local_features, (np.prod(local_features.shape[:-1]), local_features.shape[3]))\n","        local_features = normalize(local_features)  \n","      \n","        print('SET UP ENCODING')\n","        #clustering\n","        if coding == 'IFK':\n","          cluster =  GaussianMixture(n_components=par1, covariance_type='diag', max_iter=1000).fit(local_features)\n","        else:\n","          cluster = MiniBatchKMeans(n_clusters=par1,\n","                                    random_state=0,\n","                                    batch_size=2000)\n","        cluster.fit(local_features)\n","\n","        del local_features\n","\n","      print('GENERATE FEATURES')\n","      import time\n","      time.sleep(1) \n","      \n","      #bovw\n","      x_train = []\n","      y_train = []\n","\n","      for i in range(2):\n","        x_train_, y_train_ = get_cnn_features(train_generator, model)\n","        x_val_, y_val_ = get_cnn_features(validation_generator, model)\n","\n","        x_train_ = np.concatenate((x_train_, x_val_), axis=0)\n","        y_train_ = np.concatenate((y_train_, y_val_), axis=0)\n","\n","        for x, y in zip(x_train_, y_train_):\n","          x_train.append(x)\n","          y_train.append(y)\n","\n","\n","      x_train = np.asarray(x_train)\n","      y_train = np.asarray(y_train)\n","      \n","      print('GET CODED FEATURES')\n","      if coding != 'None':\n","        x_train = get_coded_features(coding, x_train, cluster)\n","        \n","      else: \n","        x_train = np.reshape(x_train, (x_train.shape[0], np.prod(x_train.shape[1:])))\n","        \n","      print(x_train.shape)\n","      \n","      \n","      print('FIT SVM')\n","      #classifier\n","      clf = LinearSVC(max_iter=500*5000, C = 1, multi_class='ovr')\n","      clf.fit(x_train, y_train)\n","      \n","      del x_train\n","      del y_train\n","      print('PREDICTIONS')\n","      #predict\n","      \n","      x_test, y_test = get_cnn_features(test_generator, model)\n","\n","      if coding == 'BOW':\n","\n","\n","        x_test = get_coded_features(coding, x_test, cluster)\n","\n","        predictions = clf.predict(x_test)\n","        labels = y_test      \n","        \n","      else:\n","        \n","        predictions = []\n","        labels = []\n","\n","        for i in tqdm(range(int(np.ceil(test_generator.samples/test_generator.batch_size)))):\n","          \n","          \n","          x_test, y_test = test_generator.next()\n","          x_test = model.predict(x_test)\n","          \n","          if coding == 'None':\n","            x_test = np.reshape(x_test, (x_test.shape[0], np.prod(x_test.shape[1:])))\n","          else:\n","            x_test = get_coded_features(coding, x_test, cluster)\n","\n","\n","          for pred, label in zip(clf.predict(x_test), y_test):\n","            predictions.append(pred)\n","            labels.append(label)\n","\n","        predictions = np.asarray(predictions)\n","        labels = np.asarray(labels)\n","        \n","        data = [predictions, labels]\n","        \n","        \n","        with open(PATH_df + str(num) + 'filename.pickle', 'wb') as handle:\n","          pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","      prediction_lists.append(predictions)\n","      label_lists.append(labels)\n","\n","    precision, recall, fscore, support, accuracy, kappa =  eval(prediction_lists, label_lists)\n","    labels = np.asarray(label_lists).flatten()\n","    predictions = np.asarray(prediction_lists).flatten()\n","\n","    obs = pd.DataFrame([[network, coding, par1, precision, recall, fscore, support, accuracy, kappa, labels, predictions]], columns=columns)\n","    df = df.append(obs)\n","    obs\n","    \n","  df.to_pickle(PATH_df + df_file_name)\n","    \n","  "],"execution_count":0,"outputs":[{"output_type":"stream","text":["CODING:  IFK\n","PARAMETER:  100\n","FOLD 3 out of 10\n","VGG DATA GENERATOR\n","gdrive/My Drive/Colab Notebooks/Sint_Maarten/material/10/roof_material_20_0_9.zip\n","/tmp/roof_material_20_0_9\n","Found 814 images belonging to 3 classes.\n","Found 351 images belonging to 3 classes.\n","Found 10496 images belonging to 3 classes.\n","(814, 14, 14, 512)\n","SET UP ENCODING\n","GENERATE FEATURES\n","GET CODED FEATURES\n","(2330, 102400)\n","FIT SVM\n","PREDICTIONS\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 105/105 [1:02:29<00:00, 35.49s/it]\n"],"name":"stderr"}]},{"metadata":{"id":"gilUOr7rnQIa","colab_type":"code","colab":{}},"cell_type":"code","source":["df.to_pickle(PATH_df + df_file_name)\n","# df_file_name"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BiNLqgOcakk8","colab_type":"code","colab":{}},"cell_type":"code","source":["df.to_pickle(PATH_df + 'NEW_CODING_MATERIAL')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"L91WmokuA_Y0","colab_type":"code","colab":{}},"cell_type":"code","source":["prediction_lists = []\n","label_lists = []\n","\n","\n","for num in range(0,10):\n","  \n","  if num == 0:\n","    num = 10\n","\n","  with open(PATH_df +str(num) + 'filename.pickle', 'rb') as handle:\n","      predictions, labels = pickle.load(handle)\n","\n","  prediction_lists.append(predictions)\n","  label_lists.append(labels)\n","\n","\n","precision, recall, fscore, support, accuracy, kappa =  eval(prediction_lists, label_lists)\n","labels = np.asarray(label_lists, int).flatten()\n","predictions = np.asarray(prediction_lists, int).flatten()\n","obs = pd.DataFrame([[network, coding, par1, precision, recall, fscore, support, accuracy, kappa, labels, predictions]], columns=columns)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"u9Ffq66-HpO4","colab_type":"code","colab":{}},"cell_type":"code","source":["df = df.append(obs)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4MlMgvL7Ec-W","colab_type":"code","colab":{}},"cell_type":"code","source":["# df = df.reset_index().drop([4,5,6]).drop('index', axis=1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ljIhP8-iHINQ","colab_type":"code","colab":{}},"cell_type":"code","source":["df= df.reset_index().drop(4).drop('index', axis=1).reset_index().drop('index', axis=1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0z2BejRrIits","colab_type":"code","outputId":"3db49bd3-e491-4c45-cdf7-e0fc9ac7f77d","executionInfo":{"status":"ok","timestamp":1549378138520,"user_tz":-60,"elapsed":1053,"user":{"displayName":"Bart van Driel","photoUrl":"","userId":"17232338587989471094"}},"colab":{"base_uri":"https://localhost:8080/","height":289}},"cell_type":"code","source":["df"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>network</th>\n","      <th>coding</th>\n","      <th>par1</th>\n","      <th>precision</th>\n","      <th>recall</th>\n","      <th>fscore</th>\n","      <th>support</th>\n","      <th>accuracy</th>\n","      <th>kappa</th>\n","      <th>labels</th>\n","      <th>predictions</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>VGG</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>[[0.5999222000346907, 0.7078168332423942, 0.49...</td>\n","      <td>[[0.5312932226832641, 0.7754704630993291, 0.40...</td>\n","      <td>[[0.561654158049029, 0.7395791150298361, 0.441...</td>\n","      <td>[[2892.0, 6111.0, 1493.0], [0.0, 0.0, 0.0]]</td>\n","      <td>[0.6551829268292683, 0.009312339393659967]</td>\n","      <td>[0.366147638339816, 0.021664362504185877]</td>\n","      <td>[1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 2, 0, 2, ...</td>\n","      <td>[1, 2, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 2, 1, 2, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>VGG</td>\n","      <td>BOW</td>\n","      <td>1000</td>\n","      <td>[[0.5957306458636051, 0.6977654839490453, 0.53...</td>\n","      <td>[[0.5091632088520055, 0.7962853870070364, 0.37...</td>\n","      <td>[[0.5478961969052729, 0.7435175716736466, 0.43...</td>\n","      <td>[[2892.0, 6111.0, 1493.0], [0.0, 0.0, 0.0]]</td>\n","      <td>[0.6566120426829268, 0.010906050753588917]</td>\n","      <td>[0.35680089814496785, 0.017198931495166235]</td>\n","      <td>[1, 1, 1, 0, 1, 0, 1, 1, 2, 1, 1, 0, 0, 1, 0, ...</td>\n","      <td>[2, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>VGG</td>\n","      <td>VLAD</td>\n","      <td>100</td>\n","      <td>[[0.6704308674339978, 0.7142208924055418, 0.61...</td>\n","      <td>[[0.533609958506224, 0.8403207331042383, 0.398...</td>\n","      <td>[[0.5913084566801288, 0.7714782418294355, 0.47...</td>\n","      <td>[[2892.0, 6111.0, 1493.0], [0.0, 0.0, 0.0]]</td>\n","      <td>[0.6929401676829269, 0.0074360974947416305]</td>\n","      <td>[0.4167208286594851, 0.012479352727029908]</td>\n","      <td>[0, 2, 0, 2, 1, 1, 1, 1, 1, 1, 0, 2, 2, 1, 1, ...</td>\n","      <td>[0, 2, 0, 1, 1, 1, 1, 1, 1, 1, 0, 2, 2, 1, 1, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>VGG</td>\n","      <td>LLC</td>\n","      <td>100</td>\n","      <td>[[0.5602261577636733, 0.6936128911824869, 0.47...</td>\n","      <td>[[0.5207814661134162, 0.7610374734086074, 0.34...</td>\n","      <td>[[0.539235652889096, 0.7255478258302017, 0.398...</td>\n","      <td>[[2892.0, 6111.0, 1493.0], [0.0, 0.0, 0.0]]</td>\n","      <td>[0.6357755335365853, 0.011473857278443161]</td>\n","      <td>[0.32841680254648453, 0.02173627771118564]</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 2, ...</td>\n","      <td>[0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>VGG</td>\n","      <td>IFK</td>\n","      <td>100</td>\n","      <td>[[0.6680810542442371, 0.6541866027265955, 0.61...</td>\n","      <td>[[0.36002766251728907, 0.9033873343151695, 0.2...</td>\n","      <td>[[0.4660079060457523, 0.7586316419707991, 0.30...</td>\n","      <td>[[2892.0, 6111.0, 1493.0], [0.0, 0.0, 0.0]]</td>\n","      <td>[0.6539348323170732, 0.00870028391845894]</td>\n","      <td>[0.2843886300725028, 0.030575889725215247]</td>\n","      <td>[0, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 0, 2, 2, 1, ...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 2, 1, 1, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  network coding  par1                                          precision  \\\n","0     VGG   None  None  [[0.5999222000346907, 0.7078168332423942, 0.49...   \n","1     VGG    BOW  1000  [[0.5957306458636051, 0.6977654839490453, 0.53...   \n","2     VGG   VLAD   100  [[0.6704308674339978, 0.7142208924055418, 0.61...   \n","3     VGG    LLC   100  [[0.5602261577636733, 0.6936128911824869, 0.47...   \n","4     VGG    IFK   100  [[0.6680810542442371, 0.6541866027265955, 0.61...   \n","\n","                                              recall  \\\n","0  [[0.5312932226832641, 0.7754704630993291, 0.40...   \n","1  [[0.5091632088520055, 0.7962853870070364, 0.37...   \n","2  [[0.533609958506224, 0.8403207331042383, 0.398...   \n","3  [[0.5207814661134162, 0.7610374734086074, 0.34...   \n","4  [[0.36002766251728907, 0.9033873343151695, 0.2...   \n","\n","                                              fscore  \\\n","0  [[0.561654158049029, 0.7395791150298361, 0.441...   \n","1  [[0.5478961969052729, 0.7435175716736466, 0.43...   \n","2  [[0.5913084566801288, 0.7714782418294355, 0.47...   \n","3  [[0.539235652889096, 0.7255478258302017, 0.398...   \n","4  [[0.4660079060457523, 0.7586316419707991, 0.30...   \n","\n","                                       support  \\\n","0  [[2892.0, 6111.0, 1493.0], [0.0, 0.0, 0.0]]   \n","1  [[2892.0, 6111.0, 1493.0], [0.0, 0.0, 0.0]]   \n","2  [[2892.0, 6111.0, 1493.0], [0.0, 0.0, 0.0]]   \n","3  [[2892.0, 6111.0, 1493.0], [0.0, 0.0, 0.0]]   \n","4  [[2892.0, 6111.0, 1493.0], [0.0, 0.0, 0.0]]   \n","\n","                                      accuracy  \\\n","0   [0.6551829268292683, 0.009312339393659967]   \n","1   [0.6566120426829268, 0.010906050753588917]   \n","2  [0.6929401676829269, 0.0074360974947416305]   \n","3   [0.6357755335365853, 0.011473857278443161]   \n","4    [0.6539348323170732, 0.00870028391845894]   \n","\n","                                         kappa  \\\n","0    [0.366147638339816, 0.021664362504185877]   \n","1  [0.35680089814496785, 0.017198931495166235]   \n","2   [0.4167208286594851, 0.012479352727029908]   \n","3   [0.32841680254648453, 0.02173627771118564]   \n","4   [0.2843886300725028, 0.030575889725215247]   \n","\n","                                              labels  \\\n","0  [1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 2, 0, 2, ...   \n","1  [1, 1, 1, 0, 1, 0, 1, 1, 2, 1, 1, 0, 0, 1, 0, ...   \n","2  [0, 2, 0, 2, 1, 1, 1, 1, 1, 1, 0, 2, 2, 1, 1, ...   \n","3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 2, ...   \n","4  [0, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 0, 2, 2, 1, ...   \n","\n","                                         predictions  \n","0  [1, 2, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 2, 1, 2, ...  \n","1  [2, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, ...  \n","2  [0, 2, 0, 1, 1, 1, 1, 1, 1, 1, 0, 2, 2, 1, 1, ...  \n","3  [0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, ...  \n","4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 2, 1, 1, ...  "]},"metadata":{"tags":[]},"execution_count":98}]}]}